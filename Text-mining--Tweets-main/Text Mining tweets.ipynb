{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Text mining on tweets <br> <font size=3.4>Dataset:tweets.text<br>Extract tweets for any user (try choosing a user who has more tweets)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import string # special operations on strings\n",
    "import spacy # language models\n",
    "from matplotlib.pyplot import imread\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-output": true,
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/urllib3/connection.py\", line 170, in _new_conn\r\n",
      "    (self._dns_host, self.port), self.timeout, **extra_kw\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/urllib3/util/connection.py\", line 73, in create_connection\r\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\r\n",
      "  File \"/opt/conda/lib/python3.7/socket.py\", line 752, in getaddrinfo\r\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\r\n",
      "socket.gaierror: [Errno -3] Temporary failure in name resolution\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 706, in urlopen\r\n",
      "    chunked=chunked,\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 382, in _make_request\r\n",
      "    self._validate_conn(conn)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 1010, in _validate_conn\r\n",
      "    conn.connect()\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/urllib3/connection.py\", line 353, in connect\r\n",
      "    conn = self._new_conn()\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/urllib3/connection.py\", line 182, in _new_conn\r\n",
      "    self, \"Failed to establish a new connection: %s\" % e\r\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fd47b3ab210>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/requests/adapters.py\", line 449, in send\r\n",
      "    timeout=timeout\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 756, in urlopen\r\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/urllib3/util/retry.py\", line 573, in increment\r\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\r\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/shortcuts-v2.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd47b3ab210>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n",
      "    \"__main__\", mod_spec)\r\n",
      "  File \"/opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\r\n",
      "    exec(code, run_globals)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/spacy/__main__.py\", line 33, in <module>\r\n",
      "    plac.call(commands[command], sys.argv[1:])\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/plac_core.py\", line 367, in call\r\n",
      "    cmd, result = parser.consume(arglist)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/plac_core.py\", line 232, in consume\r\n",
      "    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/spacy/cli/download.py\", line 44, in download\r\n",
      "    shortcuts = get_json(about.__shortcuts__, \"available shortcuts\")\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/spacy/cli/download.py\", line 95, in get_json\r\n",
      "    r = requests.get(url)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/requests/api.py\", line 76, in get\r\n",
      "    return request('get', url, params=params, **kwargs)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/requests/api.py\", line 61, in request\r\n",
      "    return session.request(method=method, url=url, **kwargs)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/requests/sessions.py\", line 542, in request\r\n",
      "    resp = self.send(prep, **send_kwargs)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/requests/sessions.py\", line 655, in send\r\n",
      "    r = adapter.send(request, **kwargs)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/requests/adapters.py\", line 516, in send\r\n",
      "    raise ConnectionError(e, request=request)\r\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/shortcuts-v2.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd47b3ab210>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "tweets=pd.read_csv(\"../input/datatweet/Tweets.txt\",error_bad_lines=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>234 tweets with \"loser\"  I feel sorry for Rosi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tweets with \"dumb\" or \"dummy\" You must admit t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>204 tweets with \"terrible\" I loved beating the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>183 tweets with \"stupid\" @michellemalkin You w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>156 tweets with \"weak\" There is no longer a Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>117 tweets with \"dope\" or \"dopey\" Dopey  @Lor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>115 tweets with \"dishonest\" A dishonest slob o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>101 tweets with \"lightweight\" I can't resist h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>92 tweets with \"incompetent\" or \"incompetence\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>91 tweets with \"boring\" The Emmys are sooooo b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>83 tweets with \"fool\" Dopey Sugar @Lord_Sugarâ€”...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>72 tweets with \"pathetic\" Sleep eyes @ChuckTod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>64 tweets with variations of \"haters and loser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>52 tweets with \"moron\" If the morons who kille...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>50 tweets with \"racist\" In that @TimeWarner ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>45 tweets with \"clown\" .@FrankLuntz, your so-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>39 tweets with \"overrated\" or \"over-rated\" \"@s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>37 tweets with \"disgusting\" Barney Frank looke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>28 tweets with \"goofy\" When Mitt Romney asked...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25 tweets with \"low rating\" or \"bad rating\" Ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18 tweets with \"no talent\" Isn't it crazy that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>15 tweets with \"lowlife\", \"low-life\", or \"low ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>106 tweets implying \"global warming\" is fake T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>84 tweets implying Obama is foreign (birtheris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>31 tweets implying \"autism\" is caused by vacci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>586 tweets with \"poll\" Even though every poll,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>421 tweets with \"money\" .@Lord_Sugar If you di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>343 tweets with \"ratings\" or \"rated\" @Toure If...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Retaliation It makes me feel so good to hit \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Isn't it crazy, I'm worth billions of dollars,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Isn't it crazy that people of little or no ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Whenever I tweet, some call it a tirade..total...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>I was viciously attacked by Mr. Khan at the De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>I study cowards and stupid people \"@MrMarin88...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Ft Lauderdale plaintiffs must pay me close to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    X\n",
       "0   234 tweets with \"loser\"  I feel sorry for Rosi...\n",
       "1   tweets with \"dumb\" or \"dummy\" You must admit t...\n",
       "2   204 tweets with \"terrible\" I loved beating the...\n",
       "3   183 tweets with \"stupid\" @michellemalkin You w...\n",
       "4                                                   .\n",
       "5   156 tweets with \"weak\" There is no longer a Be...\n",
       "6                                                   .\n",
       "7    117 tweets with \"dope\" or \"dopey\" Dopey  @Lor...\n",
       "8                                                   .\n",
       "9   115 tweets with \"dishonest\" A dishonest slob o...\n",
       "10  101 tweets with \"lightweight\" I can't resist h...\n",
       "11  92 tweets with \"incompetent\" or \"incompetence\"...\n",
       "12  91 tweets with \"boring\" The Emmys are sooooo b...\n",
       "13  83 tweets with \"fool\" Dopey Sugar @Lord_Sugarâ€”...\n",
       "14  72 tweets with \"pathetic\" Sleep eyes @ChuckTod...\n",
       "15  64 tweets with variations of \"haters and loser...\n",
       "16  52 tweets with \"moron\" If the morons who kille...\n",
       "17  50 tweets with \"racist\" In that @TimeWarner ha...\n",
       "18  45 tweets with \"clown\" .@FrankLuntz, your so-c...\n",
       "19  39 tweets with \"overrated\" or \"over-rated\" \"@s...\n",
       "20  37 tweets with \"disgusting\" Barney Frank looke...\n",
       "21   28 tweets with \"goofy\" When Mitt Romney asked...\n",
       "22  25 tweets with \"low rating\" or \"bad rating\" Ju...\n",
       "23  18 tweets with \"no talent\" Isn't it crazy that...\n",
       "24  15 tweets with \"lowlife\", \"low-life\", or \"low ...\n",
       "25  106 tweets implying \"global warming\" is fake T...\n",
       "26  84 tweets implying Obama is foreign (birtheris...\n",
       "27  31 tweets implying \"autism\" is caused by vacci...\n",
       "28  586 tweets with \"poll\" Even though every poll,...\n",
       "29  421 tweets with \"money\" .@Lord_Sugar If you di...\n",
       "30  343 tweets with \"ratings\" or \"rated\" @Toure If...\n",
       "31  Retaliation It makes me feel so good to hit \"s...\n",
       "32  Isn't it crazy, I'm worth billions of dollars,...\n",
       "33   Isn't it crazy that people of little or no ta...\n",
       "34  Whenever I tweet, some call it a tirade..total...\n",
       "35  I was viciously attacked by Mr. Khan at the De...\n",
       "36   I study cowards and stupid people \"@MrMarin88...\n",
       "37   Ft Lauderdale plaintiffs must pay me close to..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['234 tweets with \"loser\"  I feel sorry for Rosie \\'s new partner in love whose parents are devastated at the thought of their daughter being with @Rosie--a true loser. â€” Donald J. Trump (@realDonaldTrump) December 14, 2011 222.',\n",
       " 'tweets with \"dumb\" or \"dummy\" You must admit that Bryant Gumbel is one of the dumbest racists around - an arrogant dope with no talent. Failed at CBS etc-why still on TV? â€” Donald J. Trump (@realDonaldTrump) August 21, 2013 .',\n",
       " '204 tweets with \"terrible\" I loved beating these two terrible human beings. I would never recommend that anyone use her lawyer, he is a total loser! â€” Donald J. Trump (@realDonaldTrump) May 23, 2013 .',\n",
       " '183 tweets with \"stupid\" @michellemalkin You were born stupid! â€” Donald J. Trump (@realDonaldTrump) March 22, 2013',\n",
       " '.',\n",
       " '156 tweets with \"weak\" There is no longer a Bernie Sanders \"political revolution.\" He is turning out to be a weak and somewhat pathetic figure,wants it all to end! â€” Donald J. Trump (@realDonaldTrump) July 24, 2016',\n",
       " '.',\n",
       " '117 tweets with \"dope\" or \"dopey\" Dopey  @Lord_Sugar I\\'m worth $8 billion and you\\'re worth peanuts...without my show nobody would even know who you are. â€” Donald J. Trump (@realDonaldTrump) December 7, 2012',\n",
       " '.',\n",
       " '115 tweets with \"dishonest\" A dishonest slob of a reporter, who doesn\\'t understand my sarcasm when talking about him or his wife, wrote a foolish & boring Trump \"hit\" â€” Donald J. Trump (@realDonaldTrump) February 15, 2014 .']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = [X.strip() for X in tweets.X] # remove both the leading and the trailing characters\n",
    "tweets = [X for X in tweets if X] # removes empty strings, because they are considered in Python as False\n",
    "tweets[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the list into one string/text\n",
    "text = ' '.join(tweets)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Punctuation\n",
    "no_punc_text = text.translate(str.maketrans('', '', string.punctuation)) #with arguments (x, y, z) where 'x' and 'y'\n",
    "# must be equal-length strings and characters in 'x'\n",
    "# are replaced by characters in 'y'. 'z'\n",
    "# is a string (string.punctuation here)\n",
    "no_punc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['234', 'tweets', 'with', 'loser', 'I', 'feel', 'sorry', 'for', 'Rosie', 's', 'new', 'partner', 'in', 'love', 'whose', 'parents', 'are', 'devastated', 'at', 'the', 'thought', 'of', 'their', 'daughter', 'being', 'with', 'Rosiea', 'true', 'loser', 'â€”', 'Donald', 'J', 'Trump', 'realDonaldTrump', 'December', '14', '2011', '222', 'tweets', 'with', 'dumb', 'or', 'dummy', 'You', 'must', 'admit', 'that', 'Bryant', 'Gumbel', 'is']\n"
     ]
    }
   ],
   "source": [
    "#Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "text_tokens = word_tokenize(no_punc_text)\n",
    "print(text_tokens[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1282"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n",
      "['234', 'tweets', 'loser', 'I', 'feel', 'sorry', 'Rosie', 'new', 'partner', 'love', 'whose', 'parents', 'devastated', 'thought', 'daughter', 'Rosiea', 'true', 'loser', 'â€”', 'Donald', 'J', 'Trump', 'realDonaldTrump', 'December', '14', '2011', '222', 'tweets', 'dumb', 'dummy', 'You', 'must', 'admit', 'Bryant', 'Gumbel', 'one', 'dumbest', 'racists', 'around', 'arrogant']\n"
     ]
    }
   ],
   "source": [
    "#Remove stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "my_stop_words = stopwords.words('english')\n",
    "my_stop_words.append('the')\n",
    "no_stop_tokens = [word for word in text_tokens if not word in my_stop_words]\n",
    "print(no_stop_tokens[0:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['234', 'tweets', 'loser', 'i', 'feel', 'sorry', 'rosie', 'new', 'partner', 'love', 'whose', 'parents', 'devastated', 'thought', 'daughter', 'rosiea', 'true', 'loser', 'â€”', 'donald', 'j', 'trump', 'realdonaldtrump', 'december', '14']\n"
     ]
    }
   ],
   "source": [
    "#Noramalize the data\n",
    "lower_words = [x.lower() for x in no_stop_tokens]\n",
    "print(lower_words[0:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['234', 'tweet', 'loser', 'i', 'feel', 'sorri', 'rosi', 'new', 'partner', 'love', 'whose', 'parent', 'devast', 'thought', 'daughter', 'rosiea', 'true', 'loser', 'â€”', 'donald', 'j', 'trump', 'realdonaldtrump', 'decemb', '14', '2011', '222', 'tweet', 'dumb', 'dummi', 'you', 'must', 'admit', 'bryant', 'gumbel', 'one', 'dumbest', 'racist', 'around', 'arrog']\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "stemmed_tokens = [ps.stem(word) for word in lower_words]\n",
    "print(stemmed_tokens[0:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP english language model of spacy library\n",
    "nlp = spacy.load('en') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234 tweets loser I feel sorry Rosie new partner love whose parents devastated thought daughter Rosiea true loser â€” Donald J Trump realDonaldTrump December 14 2011 222 tweets dumb dummy You must admit Bryant Gumbel one dumbest racists around arrogant\n"
     ]
    }
   ],
   "source": [
    "# lemmas being one of them, but mostly POS, which will follow later\n",
    "doc = nlp(' '.join(no_stop_tokens))\n",
    "print(doc[0:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['234', 'tweet', 'loser', '-PRON-', 'feel', 'sorry', 'Rosie', 'new', 'partner', 'love', 'whose', 'parent', 'devastate', 'think', 'daughter', 'Rosiea', 'true', 'loser', 'â€”', 'Donald', 'J', 'Trump', 'realDonaldTrump', 'December', '14']\n"
     ]
    }
   ],
   "source": [
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas[0:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'234': 24, 'tweet': 394, 'loser': 241, 'pron': 307, 'feel': 160, 'sorry': 353, 'rosie': 327, 'new': 272, 'partner': 289, 'love': 243, 'whose': 423, 'parent': 288, 'devastate': 127, 'think': 377, 'daughter': 123, 'rosiea': 328, 'true': 389, 'donald': 134, 'trump': 390, 'realdonaldtrump': 315, 'december': 125, '14': 7, '2011': 13, '222': 22, 'dumb': 138, 'dummy': 140, 'must': 267, 'admit': 47, 'bryant': 91, 'gumbel': 184, 'one': 283, 'dumbest': 139, 'racist': 311, 'around': 58, 'arrogant': 59, 'dope': 135, 'talent': 368, 'fail': 154, 'cbs': 98, 'etcwhy': 149, 'still': 357, 'tv': 393, 'august': 63, '21': 20, '2013': 15, '204': 19, 'terrible': 373, 'beat': 75, 'two': 396, 'human': 196, 'being': 77, 'would': 430, 'never': 271, 'recommend': 317, 'anyone': 57, 'use': 400, 'lawyer': 231, 'total': 385, 'may': 252, '23': 23, '183': 11, 'stupid': 361, 'michellemalkin': 258, 'bear': 74, 'march': 250, '22': 21, '156': 9, 'weak': 414, 'there': 375, 'long': 238, 'bernie': 78, 'sanders': 329, 'political': 301, 'revolution': 324, 'turn': 392, 'somewhat': 351, 'pathetic': 291, 'figurewant': 162, 'end': 144, 'july': 219, '24': 25, '2016': 18, '117': 5, 'dopey': 136, 'lordsugar': 240, 'be': 73, 'worth': 429, 'billion': 81, 'peanutswithout': 294, 'show': 339, 'nobody': 274, 'even': 150, 'know': 227, '2012': 14, '115': 4, 'dishonest': 130, 'slob': 344, 'reporter': 319, 'do': 132, 'not': 276, 'understand': 397, 'sarcasm': 330, 'talk': 369, 'wife': 425, 'write': 431, 'foolish': 169, 'boring': 88, 'hit': 193, 'february': 158, '15': 8, '2014': 16, '101': 2, 'lightweight': 235, 'can': 95, 'resist': 320, 'dannyzuker': 121, 'verbally': 403, 'start': 355, 'justso': 222, 'easy': 141, 'june': 220, '12': 6, '92': 45, 'incompetent': 204, 'incompetence': 203, 'say': 331, 'week': 416, 'president': 305, 'obama': 280, 'stop': 358, 'flight': 165, 'west': 418, 'africa': 49, 'so': 346, 'simple': 340, 'refuse': 318, 'october': 281, '91': 44, 'bore': 87, 'the': 374, 'emmys': 142, 'sooooo': 352, 'go': 179, 'watch': 412, 'football': 170, 'already': 52, 'winner': 428, 'good': 180, 'night': 273, 'september': 336, '83': 42, 'fool': 168, 'sugar': 364, 'bad': 68, 'kind': 226, '10': 0, '72': 41, 'sleep': 343, 'eye': 153, 'chucktodd': 104, 'kill': 225, 'meet': 254, 'press': 306, '2015': 17, '64': 40, 'variation': 402, 'hater': 187, 'iq': 212, 'high': 191, 'please': 300, 'insecureit': 207, 'fault': 156, '52': 38, 'moron': 263, 'if': 197, 'people': 295, 'charlie': 101, 'hebdo': 190, 'wait': 409, 'magazine': 246, 'fold': 167, 'money': 260, 'success': 363, 'january': 214, '50': 37, 'in': 201, 'timewarner': 382, 'hbo': 188, 'really': 316, 'gumbeland': 185, 'mean': 253, 'fire': 164, 'switch': 366, 'bldgs': 84, '30': 29, '45': 36, 'clown': 107, 'frankluntz': 174, 'socalle': 347, 'focus': 166, 'group': 183, 'joke': 216, 'come': 110, 'office': 282, 'look': 239, 'business': 92, '39': 33, 'overrate': 286, 'scottygam': 333, 'jon': 217, 'stewart': 356, 'call': 93, 'fckface': 157, 'von': 407, 'clownstick': 108, 'what': 420, 'overrated': 287, 'assholetotal': 61, 'phoney': 297, '37': 32, 'disgust': 128, 'barney': 71, 'frank': 173, 'disgustingnipple': 129, 'protrudingin': 308, 'blue': 86, 'shirt': 337, 'congress': 112, 'very': 404, 'disrespectful': 131, '28': 27, 'goofy': 181, 'when': 421, 'mitt': 259, 'romney': 326, 'ask': 60, 'endorsement': 145, 'last': 229, 'time': 381, 'awkward': 65, 'could': 114, 'win': 427, '25': 26, 'low': 244, 'rate': 312, 'rating': 314, 'just': 221, 'hear': 189, 'crazy': 116, 'morningmika': 262, 'mental': 256, 'breakdown': 89, 'morningjoe': 261, 'joe': 215, 'mess': 257, '18': 10, 'little': 237, 'critical': 119, 'accomplishment': 46, 'great': 182, 'retribution': 323, 'lowlife': 245, 'life': 234, 'some': 348, 'journalist': 218, 'claim': 105, 'make': 247, 'pass': 290, '29': 28, 'year': 432, 'ago': 50, 'happen': 186, 'like': 236, 'nytime': 279, 'story': 359, 'become': 76, '19': 12, '106': 3, 'imply': 199, 'global': 178, 'warming': 411, 'fake': 155, 'concept': 111, 'create': 117, 'chinese': 103, 'order': 285, 'us': 399, 'manufacturing': 248, 'noncompetitive': 275, 'november': 278, '84': 43, 'foreign': 171, 'birtherism': 83, 'an': 55, 'extremely': 152, 'credible': 118, 'source': 354, 'tell': 372, 'barackobamas': 70, 'birth': 82, 'certificate': 99, 'fraud': 175, '31': 30, 'autism': 64, 'cause': 97, 'vaccine': 401, 'massive': 251, 'combined': 109, 'inoculation': 206, 'small': 345, 'child': 102, 'big': 80, 'increase': 205, '586': 39, 'poll': 302, 'though': 379, 'every': 151, 'drudge': 137, 'etc': 148, 'debate': 124, 'lot': 242, 'foxnews': 172, 'put': 310, 'negative': 270, 'biased': 79, '421': 35, 'ipod': 211, 'rich': 325, 'instead': 209, 'peanut': 293, '343': 31, 'toure': 387, 'file': 163, 'bankruptcynow': 69, 'retaliation': 322, 'sleazebag': 342, 'back': 66, 'much': 266, 'well': 417, 'see': 335, 'psychiatrist': 309, 'wharton': 419, 'employ': 143, 'thousand': 380, 'get': 177, 'insulted': 210, 'enough': 147, 'twitter': 395, 'dollar': 133, 'libel': 233, 'blogger': 85, 'afford': 48, 'suit': 365, 'wild': 426, 'someone': 350, 'attack': 62, 'always': 53, 'backexcept': 67, '100x': 1, 'this': 378, 'nothing': 277, 'tirade': 383, 'rather': 313, 'way': 413, 'somebody': 349, 'challenge': 100, 'unfairly': 398, 'fight': 161, 'brutal': 90, 'tough': 386, 'take': 367, 'important': 200, 'calm': 94, 'person': 296, 'scum': 334, 'positive': 303, 'subject': 362, 'whenever': 422, 'tiradetotally': 384, 'how': 195, 'anger': 56, 'impatience': 198, 'enemy': 146, 'tantrum': 370, 'meltdown': 255, 'amazing': 54, 'thing': 376, 'national': 269, 'incident': 202, 'viciously': 405, 'mr': 264, 'khan': 223, 'democratic': 126, 'convention': 113, 'allow': 51, 'respond': 321, 'hillary': 192, 'vote': 408, 'iraq': 213, 'war': 410, 'danrpriest': 122, 'curiosity': 120, 'care': 96, 'study': 360, 'coward': 115, 'mrmarin88': 265, 'why': 424, 'insist': 208, 'ppl': 304, 'name': 268, 'try': 391, 'taunt': 371, 'kid': 224, 'schoolyard': 332, 'only': 284, 'pig': 298, 'slaughter': 341, 'ft': 176, 'lauderdale': 230, 'plaintiff': 299, 'pay': 292, 'close': 106, '400k': 34, 'legal': 232, 'fee': 159, 'trial': 388, 'victory': 406, 'many': 249, 'honest': 194, 'knowingly': 228, 'basic': 72, 'shouldbe': 338, 'weed': 415}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ago', 'allow', 'already', 'always', 'amazing', 'an', 'anger', 'anyone', 'around', 'arrogant', 'ask', 'assholetotal', 'attack', 'august', 'autism', 'awkward', 'back', 'backexcept', 'bad', 'bankruptcynow', 'barackobamas', 'barney', 'basic', 'be', 'bear', 'beat', 'become', 'being', 'bernie', 'biased', 'big', 'billion', 'birth', 'birtherism', 'bldgs', 'blogger', 'blue', 'bore', 'boring', 'breakdown', 'brutal', 'bryant', 'business', 'call', 'calm', 'can', 'care', 'cause', 'cbs', 'certificate']\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names()[50:100])\n",
    "print(X.toarray()[50:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(923, 433)\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how can bigrams and trigrams can be included here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_ngram_range = CountVectorizer(analyzer='word',ngram_range=(1,3),max_features = 100)\n",
    "bow_matrix_ngram =vectorizer_ngram_range.fit_transform(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '2012', '2013', '2016', 'about', 'all', 'always', 'am', 'an', 'and', 'are', 'at', 'august', 'back', 'bad', 'be', 'billion', 'boring', 'bryant', 'but', 'by', 'can', 'crazy', 'december', 'dishonest', 'don', 'donald', 'donald trump', 'donald trump realdonaldtrump', 'dopey', 'dumb', 'feel', 'for', 'get', 'has', 'have', 'he', 'if', 'in', 'is', 'isn', 'it', 'just', 'know', 'life', 'loser', 'low', 'may', 'me', 'moron', 'my', 'never', 'no', 'no talent', 'not', 'of', 'of the', 'on', 'or', 'pathetic', 'people', 'realdonaldtrump', 'realdonaldtrump august', 'realdonaldtrump december', 'realdonaldtrump may', 'say', 'so', 'stupid', 'talent', 'that', 'that people', 'the', 'those', 'time', 'to', 'total', 'trump', 'trump realdonaldtrump', 'trump realdonaldtrump august', 'trump realdonaldtrump december', 'trump realdonaldtrump february', 'trump realdonaldtrump july', 'trump realdonaldtrump may', 'trump realdonaldtrump september', 'tweets', 'tweets implying', 'tweets with', 'very', 'was', 'what', 'when', 'which', 'who', 'whose', 'with', 'with no', 'worth', 'would', 'you', 'you are']\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 1 0]\n",
      " [0 0 1 ... 1 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer_ngram_range.get_feature_names())\n",
    "print(bow_matrix_ngram.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '10 2012', '12', '14', '15', '18', '18 2013', '18 tweets', '18 tweets with', '183', '183 tweets', '183 tweets with', '19', '19 2016', '2011', '2011 222', '2012', '2013', '2014', '2015', '2016', '204', '21', '23', '23 2013', '24', '24 2016', '92 tweets', 'about', 'about me', 'accomplishments', 'accomplishments are', 'accomplishments are great', 'again', 'all', 'always', 'am', 'an', 'and', 'and you', 'are', 'are great', 'are great with', 'around', 'at', 'at the', 'august', 'autism', 'back', 'bad', 'bad things', 'bad things about', 'be', 'be so', 'be so critical', 'been', 'billion', 'boring', 'bryant', 'bryant gumbel', 'but', 'by', 'called', 'can', 'can be', 'can be so', 'cbs', 'clown', 'come', 'crazy', 'crazy that', 'crazy that people', 'critical', 'critical of', 'critical of those', 'december', 'december 10', 'december 10 2012', 'disgusting', 'dishonest', 'don', 'donald', 'donald trump', 'donald trump realdonaldtrump', 'dope', 'dopey', 'dumb', 'dumb racist', 'dummy', 'employ', 'employ thousands', 'employ thousands of', 'etc', 'even', 'every', 'february', 'feel', 'feel so', 'fool', 'for', 'for the', 'get', 'global', 'global warming', 'good', 'goofy', 'great', 'great with', 'great with no', 'gumbel', 'had', 'has', 'haters', 'have', 'have been', 'he', 'he is', 'her', 'him', 'his', 'hit', 'if', 'if you', 'implying', 'in', 'incompetent', 'is', 'is one', 'is one of', 'isn', 'isn it', 'isn it crazy', 'it', 'it crazy', 'it crazy that', 'joke', 'joke donald', 'joke donald trump', 'july', 'just', 'know', 'life', 'lightweight', 'like', 'little', 'little or', 'little or no', 'lord_sugar', 'loser', 'loser donald', 'loser donald trump', 'losers', 'love', 'low', 'low life', 'made', 'makes', 'may', 'may 2013', 'me', 'me on', 'money', 'moron', 'morons', 'morons who', 'much', 'must', 'my', 'my office', 'never', 'no', 'no retribution', 'no talent', 'no talent or', 'not', 'nothing to do', 'november', 'november 2012', 'now', 'now worth', 'obama', 'of', 'of little', 'of little or', 'of people', 'of the', 'of those', 'of those whose', 'office', 'on', 'one', 'one of', 'one of the', 'only', 'or', 'or no', 'or no talent', 'or success', 'or success can', 'out', 'over', 'overrated', 'pathetic', 'people', 'people of', 'people of little', 'please don feel', 'poll', 'racist', 'racist moron', 'racists', 'racists around', 'racists around an', 'rated', 'rated scottygam', 'rated scottygam realdonaldtrump', 'rated toure', 'rated toure if', 'rather', 'rather way', 'rather way of', 'rating', 'rating just', 'rating just heard', 'rating or', 'rating or bad', 'ratings', 'ratings morning_joe', 'ratings morning_joe joe', 'ratings or', 'ratings or rated', 'ratings you', 'ratings you would', 're', 're worth', 're worth peanuts', 'realdonaldtrump', 'realdonaldtrump always', 'realdonaldtrump always insist', 'realdonaldtrump august', 'realdonaldtrump august 2012', 'realdonaldtrump august 2015', 'realdonaldtrump august 21', 'realdonaldtrump august 23', 'realdonaldtrump august 30', 'realdonaldtrump december', 'realdonaldtrump december 10', 'realdonaldtrump december 14', 'realdonaldtrump december 2012', 'realdonaldtrump december 21', 'realdonaldtrump february', 'realdonaldtrump february 14', 'realdonaldtrump february 15', 'realdonaldtrump february 24', 'realdonaldtrump january', 'realdonaldtrump january 14', 'realdonaldtrump jon', 'realdonaldtrump jon stewart', 'realdonaldtrump july', 'realdonaldtrump july 12', 'realdonaldtrump july 18', 'realdonaldtrump july 24', 'realdonaldtrump june', 'realdonaldtrump june 12', 'realdonaldtrump just', 'realdonaldtrump just out', 'realdonaldtrump march', 'realdonaldtrump march 22', 'realdonaldtrump may', 'realdonaldtrump may 19', 'realdonaldtrump may 2013', 'realdonaldtrump may 23', 'realdonaldtrump november', 'realdonaldtrump november 2012', 'realdonaldtrump october', 'realdonaldtrump october 24', 'realdonaldtrump september', 'realdonaldtrump september 10', 'realdonaldtrump september 2016', 'realdonaldtrump september 23', 'really', 'really dumb', 'really dumb racist', 'really rich', 'really rich instead', 'recommend', 'recommend that', 'recommend that anyone', 'refused', 'refused total', 'refused total incompetent', 'reporter', 'reporter who', 'reporter who doesn', 'resist', 'resist hitting', 'resist hitting lightweight', 'respond', 'respond hillary', 'respond hillary voted', 'retaliation', 'retaliation it', 'retaliation it makes', 'retribution', 'retribution donald', 'retribution donald trump', 'retribution when', 'retribution when someone', 'revolution', 'revolution he', 'revolution he is', 'rich', 'rich instead', 'rich instead of', 'romney', 'romney asked', 'romney asked me', 'rosie', 'rosie new', 'rosie new partner', 'rosie true', 'rosie true loser', 'sanders', 'sanders political', 'sanders political revolution', 'sarcasm', 'sarcasm when', 'sarcasm when talking', 'say', 'say bad', 'say bad things', 'say had', 'say had tantrum', 'say such', 'say such bad', 'say the', 'say the ipod', 'saying', 'saying for', 'saying for weeks', 'schoolyard', 'schoolyard only', 'schoolyard only stupid', 'scottygam', 'scottygam realdonaldtrump', 'scottygam realdonaldtrump jon', 'scum', 'scum and', 'scum and positive', 'scum they', 'scum they should', 'seeing', 'seeing psychiatrist', 'seeing psychiatrist which', 'september', 'september 10', 'september 10 2013', 'september 2016', 'september 23', 'september 23 2013', 'shirt', 'shirt before', 'shirt before congress', 'should', 'should be', 'should be weeded', 'should have', 'should have known', 'show', 'show anger', 'show anger disgust', 'show going', 'show going to', 'show nobody', 'show nobody would', 'simple', 'simple but', 'simple but he', 'slaughtered', 'slaughtered again', 'sleazebags', 'sleazebags back', 'sleazebags back much', 'sleep', 'sleep eyes', 'sleep eyes chucktodd', 'slob', 'slob of', 'slob of reporter', 'small', 'small children', 'small children is', 'so', 'so awkward', 'so awkward and', 'so called', 'so called focus', 'so critical', 'so critical of', 'so good', 'so good to', 'so much', 'so much about', 'so pathetic', 'so pathetic and', 'so simple', 'so simple but', 'so stupid', 'so stupid or', 'some', 'some are', 'some are knowingly', 'some call', 'some call it', 'some low', 'some low life', 'somebody', 'somebody challenges', 'somebody challenges you', 'someone', 'someone attacks', 'someone attacks me', 'somewhat', 'somewhat pathetic', 'somewhat pathetic figure', 'sooooo', 'sooooo boring', 'sooooo boring terrible', 'sorry', 'sorry for', 'sorry for rosie', 'sorry losers', 'stupid', 'stupid donald', 'stupid donald trump', 'stupid or', 'stupid people', 'success', 'success can', 'success can be', 'such', 'talent', 'talent or', 'talent or success', 'talking', 'talking about', 'terrible', 'that', 'that people', 'that people of', 'the', 'they', 'things', 'things about', 'those', 'those whose', 'those whose accomplishments', 'thousands', 'thousands of', 'thousands of people', 'time', 'tirade', 'to', 'total', 'total joke', 'trump', 'trump realdonaldtrump', 'trump realdonaldtrump august', 'trump realdonaldtrump december', 'trump realdonaldtrump february', 'trump realdonaldtrump july', 'trump realdonaldtrump may', 'trump realdonaldtrump september', 'tweets', 'tweets implying', 'tweets with', 'very', 'warming', 'was', 'weak', 'what', 'when', 'which', 'who', 'who can', 'whose', 'whose accomplishments', 'whose accomplishments are', 'why', 'win', 'with', 'with no', 'with no retribution', 'with no talent', 'worth', 'would', 'would have', 'you', 'you are', 'your']\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.10344537 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.11192527 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer_n_gram_max_features = TfidfVectorizer(norm=\"l2\",analyzer='word', ngram_range=(1,3), max_features = 500)\n",
    "tf_idf_matrix_n_gram_max_features =vectorizer_n_gram_max_features.fit_transform(tweets)\n",
    "print(vectorizer_n_gram_max_features.get_feature_names())\n",
    "print(tf_idf_matrix_n_gram_max_features.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generate wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "# Define a function to plot word cloud\n",
    "def plot_cloud(wordcloud):\n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(40, 30))\n",
    "    # Display image\n",
    "    plt.imshow(wordcloud) \n",
    "    # No axis details\n",
    "    plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate wordcloud\n",
    "stopwords = STOPWORDS\n",
    "stopwords.add('will')\n",
    "wordcloud = WordCloud(width = 3000, height = 2000, background_color='black', max_words=100,colormap='Set2',stopwords=stopwords).generate(text)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
